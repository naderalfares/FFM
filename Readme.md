# Free Food Map Scraper
A web scraper that scrapes the web for free food/meal resources and events. The scraper scrapes the url links and its sublinks, if they exist, for a set of keywords. The data is then stored in a csv file.

# Installation
1. Clone the repository or download the zip file.
2. Install the python libraries using the following command:
```pip3 install -r requirements.txt```
3. To find url links for specific key words, run the following command:
```python3 ScrapeForUrls.py```
This command will generate a file called "urls.txt" in the scrpy project *free_food_scraper* directory that contains the urls for the key words (using [Google Search Results](https://github.com/Nv7-GitHub/googlesearch)).
4. To run the scraper using the urls generated by the previous step, run the following command:
```scrapy crawl free_food_scraper```
Note: You will need to be in the *free_food_scraper* directory to run this command.
5. The scraper will generate a file called "results.csv", which contains the scraped data. The scraper will continue to append to this file each time it is run. To stop the scraper, you will can use the following command (key shortcut):
```Ctrl + C```
Note: you might need to run the command a few times to stop the scraper. This will force the scraper threads to stop appending to the file.

# Configuration
- The key words for the goole search can be modified in the *ScrapeForUrls.py* file.
- The key to scrape the data can be modified in the *free_food_scraper.py* file.

# TODOS
The following checkboxes are the tasks that need to be completed:
- [x] Scrape the web for urls using a set of key words.
- [x] Scrape the urls and sub-urls for the set of key words.
- [x] Store the data in a csv file.
- [ ] exctract, with proper formatting, the data such as location and time from the scraped data.


# Documentations & References
- [Google Search Results] (https://github.com/Nv7-GitHub/googlesearch)
- [Scrapy Python] (https://docs.scrapy.org/en/latest/)